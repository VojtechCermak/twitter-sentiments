{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from feature_extraction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms = ['lemmas', 'tokens']\n",
    "aggregates = ['hour', '5min', 'min', 'none']\n",
    "\n",
    "#labels\n",
    "directions = ['past', 'future']\n",
    "windows = [60, 1]\n",
    "\n",
    "embeddings = ['Twitter_200D', 'GoogleNews_300D', 'Wikipedia_300D']\n",
    "WV_vectorizers = ['mean', 'mean_sw', 'minmax', 'idf', 'idf_sw']\n",
    "BOW_vectorizers = ['binary', 'count', 'count_sw', 'frequency', 'tfidf', 'tfidf_sw', 'log_tfidf', 'log_tfidf_sw']\n",
    "\n",
    "\n",
    "# validation\n",
    "models = ['L2_logit', 'L1_logit', 'nb']\n",
    "metrics = ['kappa', 'acc']\n",
    "\n",
    "inputDict = {'forms':forms, 'aggregates':aggregates, 'directions':directions, 'windows':windows, \n",
    "             'BOW_vectorizers':BOW_vectorizers, 'WV_vectorizers':WV_vectorizers, 'embeddings':embeddings,\n",
    "             'models':models, 'metrics':metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Features(inputDict)\n",
    "f.embedding_path = {'Twitter_200D':'N:\\\\diplomka temp\\\\word2vec\\\\glove.twitter.27B.200d.txt',\n",
    "                    'GoogleNews_300D': 'N:\\\\diplomka temp\\\\word2vec\\\\GoogleNews-vectors-negative300.bin',\n",
    "                    'Wikipedia_300D':'N:\\\\diplomka temp\\\\word2vec\\\\glove.840B.300d.txt'}\n",
    "f.price_path = 'N:\\\\diplomka temp\\\\dataMarket\\\\AAPL1min.csv'\n",
    "f.tweets_path = 'N:\\\\diplomka temp\\\\dataProcessed\\\\tweetsAAPL.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f.load_data()\n",
    "f.load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating: ('lemmas', 'hour')\n",
      "Aggregating: ('lemmas', '5min')\n",
      "Aggregating: ('lemmas', 'min')\n",
      "Aggregating: ('lemmas', 'none')\n",
      "Aggregating: ('tokens', 'hour')\n",
      "Aggregating: ('tokens', '5min')\n",
      "Aggregating: ('tokens', 'min')\n",
      "Aggregating: ('tokens', 'none')\n",
      "Wall time: 47min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f.create_corpuses()\n",
    "del f.tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('lemmas', 'hour', 'Twitter_200D', 'mean')\n",
      "('lemmas', 'hour', 'Twitter_200D', 'mean_sw')\n",
      "('lemmas', 'hour', 'Twitter_200D', 'minmax')\n",
      "('lemmas', 'hour', 'Twitter_200D', 'idf')\n",
      "('lemmas', 'hour', 'Twitter_200D', 'idf_sw')\n",
      "('lemmas', 'hour', 'GoogleNews_300D', 'mean')\n",
      "('lemmas', 'hour', 'GoogleNews_300D', 'mean_sw')\n",
      "('lemmas', 'hour', 'GoogleNews_300D', 'minmax')\n",
      "('lemmas', 'hour', 'GoogleNews_300D', 'idf')\n",
      "('lemmas', 'hour', 'GoogleNews_300D', 'idf_sw')\n",
      "('lemmas', 'hour', 'Wikipedia_300D', 'mean')\n",
      "('lemmas', 'hour', 'Wikipedia_300D', 'mean_sw')\n",
      "('lemmas', 'hour', 'Wikipedia_300D', 'minmax')\n",
      "('lemmas', 'hour', 'Wikipedia_300D', 'idf')\n",
      "('lemmas', 'hour', 'Wikipedia_300D', 'idf_sw')\n",
      "('lemmas', '5min', 'Twitter_200D', 'mean')\n",
      "('lemmas', '5min', 'Twitter_200D', 'mean_sw')\n",
      "('lemmas', '5min', 'Twitter_200D', 'minmax')\n",
      "('lemmas', '5min', 'Twitter_200D', 'idf')\n",
      "('lemmas', '5min', 'Twitter_200D', 'idf_sw')\n",
      "('lemmas', '5min', 'GoogleNews_300D', 'mean')\n",
      "('lemmas', '5min', 'GoogleNews_300D', 'mean_sw')\n",
      "('lemmas', '5min', 'GoogleNews_300D', 'minmax')\n",
      "('lemmas', '5min', 'GoogleNews_300D', 'idf')\n",
      "('lemmas', '5min', 'GoogleNews_300D', 'idf_sw')\n",
      "('lemmas', '5min', 'Wikipedia_300D', 'mean')\n",
      "('lemmas', '5min', 'Wikipedia_300D', 'mean_sw')\n",
      "('lemmas', '5min', 'Wikipedia_300D', 'minmax')\n",
      "('lemmas', '5min', 'Wikipedia_300D', 'idf')\n",
      "('lemmas', '5min', 'Wikipedia_300D', 'idf_sw')\n",
      "('lemmas', 'min', 'Twitter_200D', 'mean')\n",
      "('lemmas', 'min', 'Twitter_200D', 'mean_sw')\n",
      "('lemmas', 'min', 'Twitter_200D', 'minmax')\n",
      "('lemmas', 'min', 'Twitter_200D', 'idf')\n",
      "('lemmas', 'min', 'Twitter_200D', 'idf_sw')\n",
      "('lemmas', 'min', 'GoogleNews_300D', 'mean')\n",
      "('lemmas', 'min', 'GoogleNews_300D', 'mean_sw')\n",
      "('lemmas', 'min', 'GoogleNews_300D', 'minmax')\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\IES Diplomka\\twitter-sentiments\\twitter-sentiments\\feature_extraction.py\u001b[0m in \u001b[0;36msave_VW_datasets\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m                         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVW_vectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mResults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f.saving_location = 'C:\\\\Users\\\\vojta\\\\Desktop\\\\IES Diplomka\\\\twitter-sentiments\\\\twitter-sentiments\\\\Apple_VW_datasets'\n",
    "f.save_VW_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle\n",
    "with open('N:\\\\diplomka temp\\\\dataPickled\\\\dataset_AAPL_BOW', 'rb') as handle:\n",
    "    f = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.saving_location = 'C:\\\\Users\\\\vojta\\\\Desktop\\\\IES Diplomka\\\\twitter-sentiments\\\\twitter-sentiments\\\\Apple_VW_datasets'\n",
    "self = f\n",
    "self.VW_dataset = {}\n",
    "self.VW_files = {}\n",
    "self.VW_dataset_list = []\n",
    "\n",
    "# Iterate over corpuses\n",
    "for item in self.corpus_list[:1]:\n",
    "    for emb in inputDict['embeddings']:\n",
    "        for vec in inputDict['WV_vectorizers']:\n",
    "            dataset_id = item + (emb, vec)\n",
    "            self.VW_dataset_list.append(dataset_id)\n",
    "            print(dataset_id)\n",
    "            # Save vectorized text corpus as pickle\n",
    "            text = self.corpus[item]['text']\n",
    "            embedding = self.embeddings[emb]\n",
    "            filename = str(item[0]) + '_' + str(item[1]) + '_' + str(emb) + '_' + str(vec) + '.pickle'            \n",
    "            self.VW_files[dataset_id] = filename\n",
    "            path = self.saving_location + '\\\\' + filename\n",
    "            \n",
    "            with open(path, 'wb') as handle:\n",
    "                pickle.dump(VW_vectorize(text, embedding, vec), handle)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
